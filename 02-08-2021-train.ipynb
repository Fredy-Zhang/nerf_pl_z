{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerf with GCN version\n",
    "## New adding file:\n",
    "1. models/graphConv.py: putting the GCN layer into this file, and building the adj matrix, normailze this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        ### if the adj matrix is small, it can use the mm directly.\n",
    "        # output = torch.spmm(adj, support)\n",
    "        output = torch.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Building adj matrix which using the square distance function, setting the threshold, if large than threshold setting as 0, if smaller than threshold, setting as 1.\n",
    "### 2. Based on the degree of every node, normalize the adj matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def design_adj_matrix(rays_o, threshold):\n",
    "    \"\"\"\n",
    "    input: rays_o : 3D coordinators. N_rays,\n",
    "                    Actually, it only 2D. Z is fixed.\n",
    "\n",
    "    Output:\n",
    "       return : adjacency matrix (N_rays, N_rays)\n",
    "    \"\"\"\n",
    "    dim = rays_o.shape[0]\n",
    "    # create the sparse adj matrix\n",
    "\n",
    "    y = torch.zeros(dim, dim).cuda()\n",
    "    x = torch.ones(dim, dim).cuda()\n",
    "\n",
    "    # generate the distance matrix, (dim, dim), return A + I it's a bug.\n",
    "    adj = torch.where(torch.cdist(rays_o, rays_o, p=2) <= threshold, x, y)\n",
    "#    logging.info(sum(adj,1))\n",
    "    return adj\n",
    "\n",
    "def normalize(mx): # all column will be 1.0\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    import scipy.sparse as sp\n",
    "    mx = mx.cpu()\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    mx = torch.tensor(mx).cuda()\n",
    "    return mx\n",
    "\n",
    "\n",
    "def adj_normalized(adj):\n",
    "    \"\"\"\n",
    "    Input: adj: adj matrix with (N_rays, N_rays), A+I\n",
    "\n",
    "      Avoid the information focus on some nodes which have too many neighbors. So do normalized operation.\n",
    "      degree_matrix^(-1/2) * A * degree_matrix^(-1/2), degree_ii = sum_j(Aij)\n",
    "\n",
    "    Output:\n",
    "       co-adj: (N_rays, N_rays)\n",
    "    \"\"\"\n",
    "    #degree = torch.diag(torch.sum(adj, 1) ** (-1/2))  # return the degree of every nodes\n",
    "    #return degree @ adj @ degree  # return the adj, after normalized.\n",
    "    return normalize(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adding this adj functions into models/rendering.py, because (x,y,z) already map by ndc() coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(models,\n",
    "                embeddings,\n",
    "                rays,\n",
    "                N_samples=64,\n",
    "                use_disp=False,\n",
    "                perturb=0,\n",
    "                noise_std=1,\n",
    "                N_importance=0,\n",
    "                chunk=1024*32,\n",
    "                white_back=False,\n",
    "                test_time=False\n",
    "                ):\n",
    "    def inference(model, embedding_xyz, xyz_, dir_, dir_embedded, z_vals, adj, weights_only=False):  ## add adj as new parameters\n",
    "        N_samples_ = xyz_.shape[1]\n",
    "        N_rays = xyz_.shape[0]\n",
    "        if not weights_only:\n",
    "            dir_embedded = torch.repeat_interleave(dir_embedded, repeats=N_samples_, dim=0)\n",
    "                           # (N_rays*N_samples_, embed_dir_channels)\n",
    "            ## rollback the shapre into (N_rays, N_samples, Dimension), because I want to input the MLP with the points with similar Z values.\n",
    "            ## if sample is 64, it will divide into 64 chunk of data, then input to MLP.\n",
    "            dir_embedded = dir_embedded.view(N_rays, N_samples_, 27)  \n",
    "        \n",
    "        out_chunks = []\n",
    "        for i in range(0, N_samples_):  ## putting into data points every layer.\n",
    "            xyz_embedded = embedding_xyz(xyz_[:, i, :]) # N_rays, 63\n",
    "            if not weights_only:\n",
    "                xyzdir_embedded = torch.cat([xyz_embedded,\n",
    "                                             dir_embedded[:, i, :]], 1) # N_rays, 27\n",
    "            else:\n",
    "                xyzdir_embedded = xyz_embedded\n",
    "            out_chunks += [model(xyzdir_embedded, adj, sigma_only=weights_only)]  ## put data into MLP.\n",
    "            \n",
    "        out = torch.cat(out_chunks, 0)\n",
    "        if weights_only:\n",
    "            # sigmas = out.view(N_rays, N_samples_)\n",
    "            sigmas = out.view(N_samples_, N_rays).permute(1,0)\n",
    "        else:\n",
    "            # rgbsigma = out.view(N_rays, N_samples_, 4)\n",
    "            rgbsigma = out.view(N_samples_, N_rays, 4).permute(1,0,2)   ### reshape the data.\n",
    "            rgbs = rgbsigma[..., :3] # (N_rays, N_samples_, 3)\n",
    "            sigmas = rgbsigma[..., 3] # (N_rays, N_samples_)\n",
    "        ...\n",
    "    \n",
    "    model_coarse = models[0]\n",
    "    embedding_xyz = embeddings[0]\n",
    "    embedding_dir = embeddings[1]\n",
    "\n",
    "    # Decompose the inputs\n",
    "    N_rays = rays.shape[0]\n",
    "    rays_o, rays_d = rays[:, 0:3], rays[:, 3:6] # both (N_rays, 3)\n",
    "    near, far = rays[:, 6:7], rays[:, 7:8] # both (N_rays, 1)\n",
    "\n",
    "    import math\n",
    "    adj = design_adj_matrix(rays_o, 1/math.sqrt(75))  # the threshold need to optimize.\n",
    "    co_adj = adj_normalized(adj)  ## fixed\n",
    "\n",
    "    # Embed direction\n",
    "    dir_embedded = embedding_dir(rays_d) # (N_rays, embed_dir_channels)\n",
    "    \n",
    "    ...\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the data method into single image training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Changing the datasets/llff.py. Every data comes from one image with 4096 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLFFDataset(Dataset):\n",
    "    ...\n",
    "    def read_meta(self):\n",
    "        ...\n",
    "         if self.split == 'train': \n",
    "                ...\n",
    "                self.all_rays = []\n",
    "                self.all_rgbs = []\n",
    "                rays_dict = {}\n",
    "                rgbs_dict = {}\n",
    "\n",
    "                img_paths = self.image_paths[:val_idx] + self.image_paths[val_idx+1:]  ### removing the val image from the image sets.\n",
    "                img_paths = np.array(list(img_paths) * 20)                                                     ### repeat the img names 20 times\n",
    "                np.random.shuffle(img_paths)                                                                           ### random shuffle the images\n",
    "                for i, image_path in enumerate(img_paths):\n",
    "                    if image_path not in rays_dict.keys():                                                            ### store the rays and rgbs for images, if re-choose the image, not re-calculate\n",
    "                        idx = list(self.image_paths).index(image_path)\n",
    "                        c2w = torch.FloatTensor(self.poses[idx])                                                  ### this idx of poses determined by the name of images.\n",
    "                        img = Image.open(image_path).convert('RGB')\n",
    "                            assert img.size[1]*self.img_wh[0] == img.size[0]*self.img_wh[1], \\\n",
    "                                f'''{image_path} has different aspect ratio than img_wh, \n",
    "                                    please check your data!'''\n",
    "                        img = img.resize(self.img_wh, Image.LANCZOS)\n",
    "                        img = self.transform(img) # (3, h, w)\n",
    "                        img = img.view(3, -1).permute(1, 0)\n",
    "                        \n",
    "                        self.all_rgbs += [img] # store the rgbs\n",
    "                        rgbs_dict[image_path] = img\n",
    "                        \n",
    "                        rays_o, rays_d = get_rays(self.directions, c2w) # both (h*w, 3)\n",
    "                        if not self.spheric_poses:\n",
    "                            near, far = 0, 1\n",
    "                            rays_o, rays_d = get_ndc_rays(self.img_wh[1], self.img_wh[0],\n",
    "                                                      self.focal, 1.0, rays_o, rays_d)\n",
    "                                         # near plane is always at 1.0\n",
    "                                         # near and far in NDC are always 0 and 1\n",
    "                                         # See https://github.com/bmild/nerf/issues/34\n",
    "                        else:\n",
    "                            near = self.bounds.min()\n",
    "                            far = min(8 * near, self.bounds.max()) # focus on central object only\n",
    "\n",
    "                        self.all_rays += [torch.cat([rays_o, rays_d, \n",
    "                                                 near*torch.ones_like(rays_o[:, :1]),\n",
    "                                                 far*torch.ones_like(rays_o[:, :1])],\n",
    "                                                 1)] # (h*w, 8)\n",
    "                    \n",
    "                        rays_dict[image_path] = torch.cat([rays_o, rays_d,\n",
    "                                                 near*torch.ones_like(rays_o[:, :1]),\n",
    "                                                 far*torch.ones_like(rays_o[:, :1])],\n",
    "                                                 1)                  ## store the rays\n",
    "\n",
    "                        _index = np.arange(self.all_rays[i].shape[0], dtype=np.int32)  \n",
    "                        np.random.shuffle(_index)\n",
    "\n",
    "                        self.all_rays[i] = self.all_rays[i][_index[:self.batch_size]]  ## choose the 4096 points from the whole sets.\n",
    "                        self.all_rgbs[i] = self.all_rgbs[i][_index[:self.batch_size]]\n",
    "                    else:\n",
    "                        _index = np.arange(rays_dict[image_path].shape[0], dtype=np.int32)\n",
    "                        np.random.shuffle(_index)\n",
    "\n",
    "                        self.all_rays += [rays_dict[image_path][_index[:self.batch_size]]]\n",
    "                        self.all_rgbs += [rgbs_dict[image_path][_index[:self.batch_size]]]\n",
    "\n",
    "                self.all_rays = torch.cat(self.all_rays, 0) # ((N_images-1)*h*w, 8)\n",
    "                self.all_rgbs = torch.cat(self.all_rgbs, 0) # ((N_images-1)*h*w, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Disabling the shuffle in Dataloader(), because in datasets/llff.py, I shuffle the data within 4096 points in single image already. \n",
    "### So putting into the model will be 4096 points in single image, and already shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFSystem(LightningModule):\n",
    "    ...\n",
    "    def train_dataloader(self):\n",
    "                return DataLoader(self.train_dataset,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding GCN layer into NERF.\n",
    "### 1. adding GCN layer at the end of 9 layers, and only in fine-model. models/nerf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    ...\n",
    "    def __init__(self,\n",
    "                 D=8, W=256,\n",
    "                 in_channels_xyz=63, in_channels_dir=27,\n",
    "                 skips=[4]):\n",
    "        ...\n",
    "        self.gcn = GraphConvolution(W, W)\n",
    "    \n",
    "    def forward(self, x, adj, sigma_only=False):\n",
    "        if not sigma_only:\n",
    "            input_xyz, input_dir = \\\n",
    "                torch.split(x, [self.in_channels_xyz, self.in_channels_dir], dim=-1)\n",
    "        else:\n",
    "            input_xyz = x\n",
    "\n",
    "        xyz_ = input_xyz\n",
    "        for i in range(self.D):\n",
    "            if i in self.skips:\n",
    "                xyz_ = torch.cat([input_xyz, xyz_], -1)\n",
    "            xyz_ = getattr(self, f\"xyz_encoding_{i+1}\")(xyz_)\n",
    "\n",
    "        sigma = self.sigma(xyz_)\n",
    "        if sigma_only:\n",
    "            return sigma\n",
    "\n",
    "        xyz_ = F.relu(self.gcn(xyz_, adj))  ## only one GCN layer.\n",
    "\n",
    "        xyz_encoding_final = self.xyz_encoding_final(xyz_)\n",
    "        ## changes, xyz_encoding : torch.Size([32768, 256])\n",
    "\n",
    "        dir_encoding_input = torch.cat([xyz_encoding_final, input_dir], -1)\n",
    "        dir_encoding = self.dir_encoding(dir_encoding_input)\n",
    "        rgb = self.rgb(dir_encoding)\n",
    "\n",
    "        out = torch.cat([rgb, sigma], -1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFSystem(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(NeRFSystem, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.loss = loss_dict[hparams.loss_type]()\n",
    "\n",
    "        self.embedding_xyz = Embedding(3, 10) # 10 is the default number\n",
    "        self.embedding_dir = Embedding(3, 4) # 4 is the default number\n",
    "        self.embeddings = [self.embedding_xyz, self.embedding_dir]\n",
    "\n",
    "        self.nerf_coarse = NeRF_Coarse()\n",
    "        self.models = [self.nerf_coarse]\n",
    "        if hparams.N_importance > 0:\n",
    "            self.nerf_fine = NeRF()                   ## in this model, adding the nerf layer.\n",
    "            self.models += [self.nerf_fine]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because validation is to input image as order. Avoiding this different with our training, total points in one image: 504*378=190512, if batch_size is 4096, 190512%4096=2096, the final set is 2096 points. It's a big issue to our validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if input is whole image, the length of rays is equal to whole image, we divide it into several parts based on batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeRFSystem(LightningModule):\n",
    "    ...\n",
    "    def forward(self, rays):\n",
    "        B = rays.shape[0]\n",
    "        results = defaultdict(list)\n",
    "        _index = None\n",
    "        _FLAG = B == (self.hparams.img_wh[0] * self.hparams.img_wh[1])\n",
    "        \n",
    "        if _FLAG:\n",
    "            # val part only val B will reach width * height.\n",
    "            _index = np.arange(B, dtype=np.int32)\n",
    "            np.random.shuffle(_index)\n",
    "            \n",
    "        for i in range(0, B, self.hparams.chunk):\n",
    "            if _FLAG:\n",
    "                if i+self.hparams.chunk > B:\n",
    "                    _rays = rays[_index[i:B]]\n",
    "                else:\n",
    "                    _rays = rays[_index[i:i+self.hparams.chunk]]\n",
    "            else:\n",
    "                _rays = rays[i:i+self.hparams.chunk]\n",
    "            rendered_ray_chunks = \\\n",
    "                render_rays(self.models,\n",
    "                            self.embeddings,\n",
    "                            _rays,\n",
    "                            self.hparams.N_samples,\n",
    "                            self.hparams.use_disp,\n",
    "                            self.hparams.perturb,\n",
    "                            self.hparams.noise_std,\n",
    "                            self.hparams.N_importance,\n",
    "                            self.hparams.chunk, # chunk size is effective in val mode\n",
    "                            self.train_dataset.white_back)\n",
    "\n",
    "            for k, v in rendered_ray_chunks.items():\n",
    "                results[k] += [v]\n",
    "\n",
    "        for k, v in results.items():\n",
    "            results[k] = torch.cat(v, 0)\n",
    "            if _FLAG:\n",
    "                ## build the tuple\n",
    "                results[k] = torch.tensor(list(map(lambda x:x[1].tolist(), sorted(zip(_index, results[k]),\n",
    "                                                                     key=lambda t:t[0])))).cuda()\n",
    "        return results\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1 try to solve this problem, 190512/3024=63, using 3024 as the batch_size try to get the performance.\n",
    "---\n",
    "### 1. python train.py --dataset_name llff --root_dir \"/data/gpfs/projects/punim1006/zzk/dataset/nerf_llff_data/fern\"   --N_importance 64 --img_wh \"504\" \"378\"   --num_epochs \"3000\" --batch_size 3024   --optimizer adam --lr 5e-4   --lr_scheduler cosine   --exp_name \"fern_case\"    --num_gpus 4    --chunk 3024\n",
    "---\n",
    "### 2. python train.py    --dataset_name llff    --root_dir \"/data/gpfs/projects/punim1006/zzk/dataset/nerf_llff_data/flower\"    --N_importance 64 --img_wh \"504\" \"378\"    --num_epochs \"3000\" --batch_size 3024    --optimizer adam --lr 5e-4    --lr_scheduler cosine    --exp_name \"flower_l\"    --num_gpus 4    --chunk 3024 \n",
    "---\n",
    "### 3. python train.py    --dataset_name llff    --root_dir \"/data/gpfs/projects/punim1006/zzk/dataset/nerf_llff_data/fortress\"    --N_importance 64 --img_wh \"504\" \"378\"    --num_epochs \"3000\" --batch_size 3024    --optimizer adam --lr 5e-4    --lr_scheduler cosine    --exp_name \"fortress_l\"    --num_gpus 4    --chunk 3024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Adding the reference case:\n",
    "---\n",
    "### all are same, only MLP does not have GCN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf_pll",
   "language": "python",
   "name": "nerf_pll"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
